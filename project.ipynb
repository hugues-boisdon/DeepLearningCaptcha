{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lx4jEs_D3ATI"
   },
   "source": [
    "# ***Google Captcha image recognition***\n",
    "**A Deep Learning Project using TensorFlow**\n",
    "\n",
    "*by Emma Begard, Augustin Bouveau, Gabin Jobert--Rollin, Hugues Boisdon*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I Data **Fetching**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our source Dataset can be found at : *https://www.kaggle.com/datasets/mikhailma/test-dataset*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***credits : Mike Mazurov***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.1 **Dowloading** Dataset Files from ***KaggleHub*** Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VFMAVdi34rRi",
    "outputId": "04b8fd29-b158-4cd8-a1f6-f691de58d0d3"
   },
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "DATA_FOLDER_PATH_IF_CACHED = kagglehub.dataset_download(\"mikhailma/test-dataset\")\n",
    "print(\"Path to dataset files in cache:\", DATA_FOLDER_PATH_IF_CACHED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data files are downloaded in the Users **cache** by default.\n",
    "If the data source folder is moved, ***please update*** the following *path variable*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER_PATH_IF_MOVED = \"\" # Data folder path if data was moved since download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataSourceFolderPath() -> str:\n",
    "    return DATA_FOLDER_PATH_IF_CACHED if DATA_FOLDER_PATH_IF_MOVED == \"\" else DATA_FOLDER_PATH_IF_MOVED\n",
    "\n",
    "def getImagesDataFolderPath() -> str:\n",
    "    return getDataSourceFolderPath() +\"/Google_Recaptcha_V2_Images_Dataset/images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "path = DATA_FOLDER_PATH_IF_CACHED if DATA_FOLDER_PATH_IF_MOVED == \"\" else DATA_FOLDER_PATH_IF_MOVED\n",
    "Image.open(getImagesDataFolderPath() +\"/Bicycle/Bicycle (1).png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.2 First Try at **Loading** the Training and Validation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "IMG_DIMENSIONS = (120, 120) # pixels per pixels\n",
    "SEED_RANDOM = 123\n",
    "\n",
    "VALIDATION_RATIO = 0.2\n",
    "\n",
    "def getDatasets(batch_size=BATCH_SIZE, \n",
    "                img_dims=IMG_DIMENSIONS, \n",
    "                validation_ratio=VALIDATION_RATIO, \n",
    "                seed=SEED_RANDOM) -> tuple:\n",
    "  train = image_dataset_from_directory(\n",
    "    getImagesDataFolderPath(),\n",
    "    validation_split= validation_ratio,\n",
    "    subset= \"training\",\n",
    "    \n",
    "    seed=       seed,\n",
    "    image_size= img_dims,\n",
    "    batch_size= batch_size)\n",
    "\n",
    "  validation = image_dataset_from_directory(\n",
    "    getImagesDataFolderPath(),\n",
    "    validation_split= validation_ratio,\n",
    "    subset= \"validation\",\n",
    "    \n",
    "    seed=       seed,\n",
    "    image_size= img_dims,\n",
    "    batch_size= batch_size)\n",
    "  \n",
    "  return train, validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **keras** sublibrary of ***TensorFlow*** allow us to directly load our datasets (and ensure the size normalization of 120px per 120px for all images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, validation_dataset = getDatasets()\n",
    "\n",
    "CLASS_NAMES = train_dataset.class_names\n",
    "print(CLASS_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is going to be guessing between these classes for each image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Sample for Visualization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_dataset.take(1):\n",
    "  for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "    plt.title(CLASS_NAMES[labels[i]])\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II Data **Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Preprocessing layers & Optimizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 **Normalization Layer** for pixel values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Rescaling\n",
    "\n",
    "def getNormalizationLayer():\n",
    "    return Rescaling(1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pixel values will now be bound from 0 to 1 instead of 0 to 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Data Augmentation Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomZoom\n",
    "\n",
    "def getAugmentationLayers(maxRotation:float= 0.1, maxZoom:float= 0.1) -> Sequential:\n",
    "    return Sequential(\n",
    "    [\n",
    "        RandomFlip(\"horizontal\",\n",
    "                        input_shape=(IMG_DIMENSIONS[0],\n",
    "                                    IMG_DIMENSIONS[1],\n",
    "                                    3)),\n",
    "        RandomRotation(maxRotation),\n",
    "        RandomZoom(maxZoom),\n",
    "    ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for images, _ in train_dataset.take(1):\n",
    "  for i in range(9):\n",
    "    augmented_images = getAugmentationLayers()(images)\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 **Optimizations** of Data memory caching, availibity and randomization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.data import AUTOTUNE\n",
    "\n",
    "train_dataset_opti        = train_dataset.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "validation_dataset_opti   = validation_dataset.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Organizing & Rebalancing the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Creating sub folders for test, train and validation for the original dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, whithout balancing the number of images per class, our model was unable to predict some classes at all, as had a lot less images than other. To counter this effect we duplicated images from the weaker classes to balance everything. However, when setting the datasets, we might have had duplicated images in both training and validation, hence we needed to separate the original dataset into three folders (training, validation, test), to be sure teh model did not validate on already seen images. \n",
    "Then to still balance our classes, we apply the balancing function to the training dataset, and validation too, to have enought images for the validation setp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "def split_dataset_by_class(source_dir, train_dir, val_dir, test_dir, train_size=0.7, val_size=0.2, test_size=0.1, min_images=10):\n",
    "    # Ensure the directories for train, validation, and test exist\n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "    os.makedirs(val_dir, exist_ok=True)\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "    # Traverse each subdirectory (class folder) in the source directory\n",
    "    for class_name in os.listdir(source_dir):\n",
    "        class_folder = os.path.join(source_dir, class_name)\n",
    "        \n",
    "        # Skip non-directories (just in case there are files in the source dir)\n",
    "        if not os.path.isdir(class_folder):\n",
    "            continue\n",
    "\n",
    "        # Get all image files in the class folder\n",
    "        image_files = [os.path.join(class_folder, file) for file in os.listdir(class_folder)\n",
    "                       if file.lower().endswith(('png', 'jpg', 'jpeg'))]  # Adjust for your image file extensions\n",
    "\n",
    "        # Skip the class if there are fewer than min_images\n",
    "        if len(image_files) < min_images:\n",
    "            print(f\"Skipping class {class_name} because it has fewer than {min_images} images.\")\n",
    "            continue\n",
    "\n",
    "        # Create the same class subdirectories in train, val, and test directories\n",
    "        os.makedirs(os.path.join(train_dir, class_name), exist_ok=True)\n",
    "        os.makedirs(os.path.join(val_dir, class_name), exist_ok=True)\n",
    "        os.makedirs(os.path.join(test_dir, class_name), exist_ok=True)\n",
    "\n",
    "        # Shuffle the image files for randomness\n",
    "        random.shuffle(image_files)\n",
    "\n",
    "        # Calculate the number of images for each set\n",
    "        total_images = len(image_files)\n",
    "        train_count = int(total_images * train_size)\n",
    "        val_count = int(total_images * val_size)\n",
    "        test_count = total_images - train_count - val_count  # Remaining for test\n",
    "\n",
    "        # Split the images\n",
    "        train_images = image_files[:train_count]\n",
    "        val_images = image_files[train_count:train_count + val_count]\n",
    "        test_images = image_files[train_count + val_count:]\n",
    "\n",
    "        # Copy the images into the corresponding directories\n",
    "        def copy_images(image_list, target_dir):\n",
    "            for img_path in image_list:\n",
    "                shutil.copy(img_path, target_dir)\n",
    "\n",
    "        copy_images(train_images, os.path.join(train_dir, class_name))\n",
    "        copy_images(val_images, os.path.join(val_dir, class_name))\n",
    "        copy_images(test_images, os.path.join(test_dir, class_name))\n",
    "\n",
    "        print(f\"Class {class_name}: {train_count} for training, {val_count} for validation, {test_count} for testing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Balancing classes in the training folder and validation folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "def duplicate_images_randomly_per_folder(root_dir, target_count_per_folder, min_img=5, max_img=100):\n",
    "    # Traverse all subdirectories in the root directory\n",
    "    for root, subdirs, files in os.walk(root_dir):\n",
    "        # Skip the root directory itself\n",
    "        if root == root_dir:  \n",
    "            continue\n",
    "\n",
    "        # Print the current directory being processed (for debugging)\n",
    "        print(f\"Processing folder: {root}\")\n",
    "        \n",
    "        # Filter image files (png, jpg, jpeg)\n",
    "        image_files = [os.path.join(root, file) for file in files if file.lower().endswith(('png', 'jpg', 'jpeg'))]\n",
    "        current_count = len(image_files)\n",
    "        \n",
    "        # If the number of images is less than min_img, delete the folder\n",
    "        if current_count < min_img:\n",
    "            print(f\"Directory {root} has fewer than {min_img} images, deleting folder.\")\n",
    "            # Delete all the files in the folder before removing it\n",
    "            for file in image_files:\n",
    "                os.remove(file)\n",
    "            os.rmdir(root)  # Remove the folder itself\n",
    "            continue  # Skip to the next folder\n",
    "        \n",
    "        # If the number of images exceeds max_img, randomly delete images to meet max_img limit\n",
    "        if current_count > max_img:\n",
    "            print(f\"Directory {root} has more than {max_img} images, deleting excess images.\")\n",
    "            images_to_delete = current_count - max_img\n",
    "            random.shuffle(image_files)  # Shuffle to delete images randomly\n",
    "            for img_path in image_files[:images_to_delete]:\n",
    "                os.remove(img_path)  # Remove the selected images\n",
    "            current_count = max_img  # Update current count after deletion\n",
    "        \n",
    "        # If the current count is already greater than or equal to the target, skip duplication\n",
    "        if current_count >= target_count_per_folder:\n",
    "            print(f\"Directory {root} already has {current_count} images, no duplication needed.\")\n",
    "            continue\n",
    "        \n",
    "        # Calculate how many more images are needed\n",
    "        images_needed = target_count_per_folder - current_count\n",
    "        print(f\"Duplicating {images_needed} images randomly in {root}.\")\n",
    "        \n",
    "        # Duplicate images randomly until the target count is reached\n",
    "        while images_needed > 0:\n",
    "            random.shuffle(image_files)  # Shuffle the list of image files\n",
    "            for img_path in image_files:\n",
    "                if images_needed <= 0:\n",
    "                    break\n",
    "                \n",
    "                # Load the image\n",
    "                img = Image.open(img_path)\n",
    "                \n",
    "                # Create a unique name for the duplicated image\n",
    "                folder_name, img_name = os.path.split(img_path)\n",
    "                duplicated_img_name = f\"{os.path.splitext(img_name)[0]}_dup{images_needed}{os.path.splitext(img_name)[1]}\"\n",
    "                duplicated_img_path = os.path.join(folder_name, duplicated_img_name)\n",
    "                \n",
    "                # Save the duplicated image with a new name\n",
    "                img.save(duplicated_img_path)\n",
    "\n",
    "                # Decrement the remaining number of images needed\n",
    "                images_needed -= 1\n",
    "\n",
    "        print(f\"Duplicated images in {root} to reach {target_count_per_folder} images.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Image preprocesssing for Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet import preprocess_input\n",
    "\n",
    "def preprocess_data_for_resnet(image, label):\n",
    "    image = preprocess_input(image)  # Apply preprocessing\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III Our differents approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Naive CNN models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III.1.A Defining and compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "first_model = Sequential([\n",
    "  Conv2D(16, 3, padding='same', activation='relu'),\n",
    "  MaxPooling2D(),\n",
    "  Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  MaxPooling2D(),\n",
    "  Conv2D(64, 3, padding='same', activation='relu'),\n",
    "  MaxPooling2D(),\n",
    "  Flatten(),\n",
    "  Dense(128, activation='relu'),\n",
    "  Dense(len(CLASS_NAMES), name=\"outputs\")\n",
    "], name = \"First_Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model had **10** layers (**3** *Conv2D* and **3** *MaxPooling2D*, **1** *Flatten* and **2** *Dense*). After training, we saw a **validation accuracy** of 0.4224 and a **validation loss** of 4.0163, with an **accuracy** of 0.9 in training. Our model is clearly overfitted and is unable to identify the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "first_model.compile(optimizer='adam',\n",
    "              loss= SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "first_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III.1.B Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS_FIRST= 10\n",
    "\n",
    "first_history = first_model.fit(\n",
    "  train_dataset,\n",
    "  validation_data=validation_dataset,\n",
    "  epochs=EPOCHS_FIRST\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III.1.C Visualizing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = first_history.history['accuracy']\n",
    "val_acc = first_history.history['val_accuracy']\n",
    "\n",
    "loss = first_history.history['loss']\n",
    "val_loss = first_history.history['val_loss']\n",
    "\n",
    "epochs_range = range(EPOCHS_FIRST)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import argmax\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "tf.math.confusion_matrix(\n",
    "    labels,\n",
    "    predictions,\n",
    "    num_classes=None,\n",
    "    weights=None,\n",
    "    dtype=tf.dtypes.int32,\n",
    "    name=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Extract true labels from the validation dataset\n",
    "y_true = []\n",
    "for images, labels in validation_dataset:\n",
    "    y_true.extend(labels.numpy())  # Convert the labels from tensor to numpy\n",
    "\n",
    "# Step 2: Get predictions from the model (do this once, not twice)\n",
    "predictions = first_model.predict(validation_dataset)\n",
    "\n",
    "# Convert predictions to class labels (assuming probabilities)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Step 3: Generate the classification report\n",
    "infos_2 = classification_report(y_true, predicted_labels)\n",
    "print(infos_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III.2.A Defining and compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "second_model = Sequential([\n",
    "  getAugmentationLayers(),\n",
    "  getNormalizationLayer(),\n",
    "  Conv2D(16, 3, padding='same', activation='relu'),\n",
    "  MaxPooling2D(),\n",
    "  Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  MaxPooling2D(),\n",
    "  Conv2D(64, 3, padding='same', activation='relu'),\n",
    "  MaxPooling2D(),\n",
    "  Dropout(0.2),\n",
    "  Flatten(),\n",
    "  Dense(128, activation='relu'),\n",
    "  Dense(len(CLASS_NAMES), name=\"outputs\")\n",
    "], name = \"Second_Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to add 3 layer at the top for data **augmentation** (1 *RandomFlip*, 1 *RandomRotation*, 1 *RandomZoom*) and 1 *Rescaling* Layer for **normalization** (from 0->255 to 0->1). We saw a change in **validation accuracy** to 0.6226 and **loss** to 1.0823, with a minimal difference with training (< 0.04). We fixed the overfitting issue and greatly improved the loss. Next, we added 1 *Dropout* Layer before the *Flatten* Layer. The hope was to futher lessen the overfitting. We can see that the f1-Score was improved but the effect is slight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "second_model.compile(optimizer='adam',\n",
    "              loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "second_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III.2.B Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS_SECOND = 15\n",
    "second_history = second_model.fit(\n",
    "  train_dataset,\n",
    "  validation_data= validation_dataset,\n",
    "  epochs=EPOCHS_SECOND\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Extract true labels from the validation dataset\n",
    "y_true = []\n",
    "for images, labels in validation_dataset:\n",
    "    y_true.extend(labels.numpy())  # Convert the labels from tensor to numpy\n",
    "\n",
    "# Step 2: Get predictions from the model (do this once, not twice)\n",
    "predictions = second_model.predict(validation_dataset)\n",
    "\n",
    "# Convert predictions to class labels (assuming probabilities)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Step 3: Generate the classification report\n",
    "infos_2 = classification_report(y_true, predicted_labels)\n",
    "print(infos_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III.2.C Visualizing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = second_history.history['accuracy']\n",
    "val_acc = second_history.history['val_accuracy']\n",
    "\n",
    "loss = second_history.history['loss']\n",
    "val_loss = second_history.history['val_loss']\n",
    "\n",
    "epochs_range = range(EPOCHS_SECOND)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.image import decode_jpeg, resize\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "def preprocess_data_for_resnet(image, label):\n",
    "    image = preprocess_input(image)  # Apply preprocessing\n",
    "    return image, label\n",
    "train_dataset, validation_dataset = getDatasets()\n",
    "\n",
    "train_dataset= train_dataset.map(preprocess_data_for_resnet).cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "validation_dataset= validation_dataset.map(preprocess_data_for_resnet).cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Testing CNN models with new preprocessings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 partie emma\n",
    "In this part we test convolutional layers models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_directory = getImagesDataFolderPath()\n",
    "train_directory = './datas/train'  \n",
    "val_directory = './datas/validation' \n",
    "test_directory = './datas/test' \n",
    "\n",
    "# Split the dataset, skipping classes with fewer than 500 images\n",
    "split_dataset_by_class(source_directory, train_directory, val_directory, test_directory, min_images=500)\n",
    "\n",
    "# Define your target number of images for each subfolder\n",
    "target_number_per_folder = 1500  # Set your target number of images per folder\n",
    "\n",
    "# Set your root image directory (which contains subfolders like 'cars', 'bicycles', etc.)\n",
    "root_image_directory = getImagesDataFolderPath() \n",
    "\n",
    "# for training \n",
    "duplicate_images_randomly_per_folder(\"./datas/training\", 1500, min_img=500, max_img=1500)\n",
    "# for validation \n",
    "duplicate_images_randomly_per_folder(\"./datas/validation\", 1500*0.2, min_img=500*0.2, max_img=1500)\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = image_dataset_from_directory(\n",
    " \"./datas/train\" ,\n",
    "  label_mode='int',\n",
    "  image_size= IMG_DIMENSIONS,\n",
    "  batch_size= BATCH_SIZE)\n",
    "\n",
    "validation_dataset = image_dataset_from_directory(\n",
    "  \"./datas/validation\" ,\n",
    "  label_mode='int',\n",
    "  image_size= IMG_DIMENSIONS,\n",
    "  batch_size= BATCH_SIZE)\n",
    "\n",
    "test_dataset = image_dataset_from_directory(\n",
    "  \"./datas/test\" ,\n",
    "  label_mode='int',\n",
    "  seed=       SEED_RANDOM,\n",
    "  image_size= IMG_DIMENSIONS,\n",
    "  batch_size= BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, GlobalAveragePooling2D\n",
    "\n",
    "model_2_1 = Sequential([\n",
    "    getAugmentationLayers(),\n",
    "    getNormalizationLayer(),\n",
    "    Conv2D(16, (3, 3), padding='same', activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
    "    GlobalAveragePooling2D(),  # Summarizes feature maps\n",
    "    Dropout(0.3),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(len(CLASS_NAMES), activation='softmax', name=\"outputs\")\n",
    "], name=\"model_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_2_2 = Sequential([\n",
    "    getAugmentationLayers(),\n",
    "    getNormalizationLayer(),\n",
    "    Conv2D(16, (3, 3), padding='same', activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
    "    MaxPooling2D((2, 2)),  # Add additional MaxPooling layer\n",
    "    Conv2D(256, (3, 3), padding='same', activation='relu'),  # Additional convolutional layer\n",
    "    MaxPooling2D((2, 2)),\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dropout(0.3),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(len(CLASS_NAMES), activation='softmax', name=\"outputs\")\n",
    "], name=\"model_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, MaxPooling2D, GlobalAveragePooling2D, \n",
    "    Dropout, Dense, BatchNormalization, Activation\n",
    ")\n",
    "\n",
    "model_2_3 = Sequential([\n",
    "    # Augmentation and normalization layers\n",
    "    getAugmentationLayers(),\n",
    "    getNormalizationLayer(),\n",
    "    \n",
    "    # Convolutional Block 1\n",
    "    Conv2D(16, (3, 3), padding='same'),\n",
    "    BatchNormalization(),  \n",
    "    Activation('relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Convolutional Block 2\n",
    "    Conv2D(32, (3, 3), padding='same'),\n",
    "    BatchNormalization(), \n",
    "    Activation('relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Convolutional Block 3\n",
    "    Conv2D(64, (3, 3), padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Convolutional Block 4\n",
    "    Conv2D(128, (3, 3), padding='same'),\n",
    "    BatchNormalization(), \n",
    "    Activation('relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Convolutional Block 5\n",
    "    Conv2D(256, (3, 3), padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Convolutional Block 6 (Newly added)\n",
    "    Conv2D(512, (3, 3), padding='same'),\n",
    "    BatchNormalization(), \n",
    "    Activation('relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    # Global average pooling to reduce spatial dimensions\n",
    "    GlobalAveragePooling2D(),\n",
    "    \n",
    "    # Fully connected layers\n",
    "    Dropout(0.4),  # to prevent overfitting\n",
    "    Dense(256, activation='relu'),  \n",
    "    Dropout(0.4),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.4),\n",
    "\n",
    "    # Output layer\n",
    "    Dense(len(CLASS_NAMES), activation='softmax', name=\"outputs\")\n",
    "], name=\"model_2_3\")\n",
    "\n",
    "model_2_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, MaxPooling2D, GlobalAveragePooling2D, \n",
    "    Dropout, Dense, BatchNormalization, Activation\n",
    ")\n",
    "\n",
    "model_2_4 = Sequential([\n",
    "    # Augmentation and normalization layers\n",
    "    getAugmentationLayers(),\n",
    "    getNormalizationLayer(),\n",
    "    \n",
    "    # Convolutional Block 1\n",
    "    Conv2D(16, (3, 3), padding='same'),\n",
    "    BatchNormalization(),  \n",
    "    Activation('relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Convolutional Block 2\n",
    "    Conv2D(32, (3, 3), padding='same'),\n",
    "    BatchNormalization(), \n",
    "    Activation('relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Convolutional Block 3\n",
    "    Conv2D(64, (3, 3), padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Convolutional Block 4\n",
    "    Conv2D(128, (3, 3), padding='same'),\n",
    "    BatchNormalization(), \n",
    "    Activation('relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Convolutional Block 5\n",
    "    Conv2D(256, (3, 3), padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Convolutional Block 6\n",
    "    Conv2D(512, (3, 3), padding='same'),\n",
    "    BatchNormalization(), \n",
    "    Activation('relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Additional Convolutional Block 7\n",
    "    Conv2D(1024, (3, 3), padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    \n",
    "    # Convolutional Block 8\n",
    "    Conv2D(2048, (3, 3), padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    \n",
    "    # Global average pooling\n",
    "    GlobalAveragePooling2D(),\n",
    "    \n",
    "    # Fully connected layers\n",
    "    Dropout(0.5),  # Increased dropout to prevent overfitting\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    # Output layer\n",
    "    Dense(len(CLASS_NAMES), activation='softmax', name=\"outputs\")\n",
    "], name=\"model_2_4\")\n",
    "\n",
    "#model_2_3.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "import tensorflow as tf \n",
    "\n",
    "model = model_2_4\n",
    "nb = \"2_4_4\"\n",
    "LR_Rate = 0.0001\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LR_Rate)\n",
    " \n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "EPOCHS= 20\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# getting the existing labels from teh training dataset\n",
    "labels = []\n",
    "for image, label in train_dataset:\n",
    "    labels.append(label)\n",
    "\n",
    "labels = np.concatenate(labels, axis=0)\n",
    "\n",
    "print(labels)\n",
    "\n",
    "# Calculate class weights from the training labels\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(labels), y=labels)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "history = model.fit(\n",
    "  train_dataset,\n",
    "  validation_data= validation_dataset,\n",
    "  class_weight=class_weight_dict,\n",
    "  epochs=EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(EPOCHS)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "def generate_label_name_mapping(dataset, class_names):\n",
    "    \"\"\"\n",
    "    Generate a dictionary mapping numeric labels to class names.\n",
    "\n",
    "    Args:\n",
    "    - dataset: A TensorFlow dataset where labels are numeric.\n",
    "    - class_names: List of class names in the same order as used for dataset creation.\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary mapping numeric labels to class names.\n",
    "    \"\"\"\n",
    "    label_counts = Counter()\n",
    "\n",
    "    # Collect numeric labels from the dataset\n",
    "    for _, labels in dataset:\n",
    "        labels = labels.numpy()  # Convert TensorFlow tensor to numpy array\n",
    "        label_counts.update(labels)\n",
    "\n",
    "    # Generate the label-to-name mapping\n",
    "    label_name_mapping = {label: class_names[label] for label in sorted(label_counts.keys())}\n",
    "    return label_name_mapping\n",
    "\n",
    "# Example usage\n",
    "# Assuming `class_names` is extracted from the dataset directory structure\n",
    "class_names =validation_dataset.class_names # Ensure classes are sorted consistently\n",
    "label_name_mapping = generate_label_name_mapping(validation_dataset, class_names)\n",
    "\n",
    "# Print the mapping\n",
    "print(\"Label-to-Class Name Mapping:\")\n",
    "for label, name in label_name_mapping.items():\n",
    "    print(f\"{label}: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "y_pred = model.predict(validation_dataset)\n",
    "y_pred_class = np.argmax(y_pred, axis=1)\n",
    "\n",
    "y_true = []\n",
    "for images, labels in validation_dataset:\n",
    "    y_true.extend(labels.numpy())\n",
    "\n",
    "classes = validation_dataset.class_names\n",
    "\n",
    "# Générer une matrice de confusion\n",
    "cm = confusion_matrix(y_true, y_pred_class)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=classes)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Step 1: Extract true labels from the validation dataset\n",
    "y_true = []\n",
    "for images, labels in validation_dataset:\n",
    "    y_true.extend(labels.numpy())  # Convert the labels from tensor to numpy\n",
    "\n",
    "# Step 2: Get predictions from the model (do this once, not twice)\n",
    "predictions = model.predict(validation_dataset)\n",
    "\n",
    "# Convert predictions to class labels (assuming probabilities)\n",
    "predicted_labels = np.argmax(predictions, axis=1)  # Get the class with the highest probability for each sample\n",
    "\n",
    "# Step 3: Generate the classification report\n",
    "report = classification_report(y_true, predicted_labels,  output_dict=True)\n",
    "report_print = classification_report(y_true, predicted_labels)\n",
    "\n",
    "print(report_print)\n",
    "\n",
    "class_f1_scores = {f\"Class {k} F1\": v['f1-score'] for k, v in report.items() if k.isdigit()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def save_class_f1_scores_to_csv(class_f1_scores, file_name):\n",
    "    \"\"\"\n",
    "    Saves a dictionary of class F1 scores to a CSV file.\n",
    "    \"\"\"\n",
    "    with open(file_name, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        # Write the header\n",
    "        writer.writerow([\"Class\", \"F1 Score\"])\n",
    "        # Write each key-value pair as a row\n",
    "        for class_name, f1_score in class_f1_scores.items():\n",
    "            writer.writerow([class_name, f1_score])\n",
    "    \n",
    "    print(f\"Class F1 scores saved to {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_csv =\"results/f1_scores/model_\"+str(nb)+\"_f1_scores.csv\"\n",
    "\n",
    "save_class_f1_scores_to_csv(class_f1_scores, file_name_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "def add_to_csv(file_name, row):\n",
    "    \"\"\"\n",
    "    Adds a row to a CSV file. If the file does not exist, it creates the file with the header.\n",
    "\n",
    "    :param file_name: Name of the CSV file.\n",
    "    :param header: List containing column names.\n",
    "    :param row: List containing a single row of data to add.\n",
    "    \"\"\"\n",
    "    file_exists = os.path.exists(file_name)\n",
    "    header = [\"Model\", \"Accuracy\", \"loss\", \"f1 score file\", \"Epochs\", \"Batch Size\", \"Optimizer\", \"learning rate\"]\n",
    "\n",
    "    with open(file_name, mode=\"a\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        # Write header if the file does not exist\n",
    "        if not file_exists:\n",
    "            writer.writerow(header)\n",
    "\n",
    "        # Write the row\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = [\"model \"+ str(nb), accuracy, loss, file_name_csv , EPOCHS , BATCH_SIZE, \"Adam\", LR_Rate]\n",
    "add_to_csv(\"./results/global/recap_models.csv\", row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results     \n",
    "In this part we managed to go from a model able to distinguish one class only, to (hardly but still) detecting the 9 different classes remaining after the data processing. \n",
    "\n",
    "<div style=\"display: flex; align-items: center;\">\n",
    "  <img src=\"./results/graphs/model_2.png\" alt=\"Model 2 Graph\" style=\"margin-right: 10px; width: 300px;\">\n",
    "  <p>For this model we have a fine accuracy around 0.7, however when looking at the file about the f1-scores per class (results/f1_scores/model_2_f1_scores.csv), we see that our model do not really differenciate well our classes.  </p>\n",
    "</div>\n",
    "\n",
    "To resolve this we made updates on teh preprocessing, and tried to increase the model's complexity, to try to grasp more differents patterns linked to the different classes. First we tried to increment the number of filters in each convolution layer. \n",
    "\n",
    "<div style=\"display: flex; align-items: center;\">\n",
    "  <img src=\"./results/graphs/model_2_2_6.png\" alt=\"Model 2 Graph\" style=\"margin-right: 10px; width: 300px;\">\n",
    "  <p>For this model we have a fine accuracy around 0.59, however when looking at the file about the f1-scores per class (results/f1_scores/model_2_2_6_f1_scores.csv), we see that our model is still unable to differenciate well our classes.  </p>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "Also we tried to add the batch normalization layer and one more convolution layer to see if it was moere efficient. \n",
    "\n",
    "<div style=\"display: flex; align-items: center;\">\n",
    "  <img src=\"./results/graphs/model_2_3_1.png\" alt=\"Model 2 Graph\" style=\"margin-right: 10px; width: 300px;\">\n",
    "  <img src=\"./results/graphs/confu_matrix_2_3_1.png\" alt=\"Model 2 Graph\" style=\"margin-right: 10px; width: 300px;\">\n",
    "  <p>For this model we have a fine accuracy around 0.70, however when looking at the file about the f1-scores per class (results/f1_scores/model_2_3_1_f1_scores.csv), we see that this model too is still unable to differenciate well our classes. Also, almost everything is a bridge.  </p>\n",
    "</div>\n",
    "\n",
    "\n",
    "Finaly we add more convolution layer to see if it was more efficient. \n",
    "\n",
    "<div style=\"display: flex; align-items: center;\">\n",
    "  <img src=\"./results/graphs/model_2_4_3.png\" alt=\"Model 2 Graph\" style=\"margin-right: 10px; width: 300px;\">\n",
    "  <img src=\"./results/graphs/confu_matrix_2_4_3.png\" alt=\"Model 2 Graph\" style=\"margin-right: 10px; width: 300px;\">\n",
    "  <p>For this model we have a fine accuracy around 0.74, however the model overfit and looking at the file about the f1-scores per class (results/f1_scores/model_2_4_4_f1_scores.csv), we see that this model too is still unable to differenciate well our classes.  But the confusion matrix is less enclined to tell everything is just one class, which make us wonder if the result is not just random</p>\n",
    "</div>\n",
    "\n",
    "Eventually as the progress wre not great on this model, we went for other architectures with more complex structures, as we spent some times on the preprocessing part of the project.\n",
    "\n",
    "Theses bad results can be attributed  to : \n",
    "  - photos in which there is more than one item, for exemple, one can be labbelled as a car, even if there is a bridge behind.To this issue we could have taken the top three predictions for each images and see if one of teh three labels is teh right one. \n",
    "\n",
    "  - too zoomed images, to the point of them being difficult to use. Like a part of a wheel for a bike is labbeled as a bike. Hence it can create special patterns for such images ovelapping with patterns found for others labels.\n",
    "  \n",
    "Also, as it is made to counter AI and attest that we are human by finding elements on the photo (which can be difficult even for us sometimes), it can explain why it is difficult to train the model on such datas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 partie augustin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "\n",
    "\n",
    "# > 0.74 Val accuracy, 0.80 removing Others\n",
    "second_model = Sequential([\n",
    "  getAugmentationLayers(),\n",
    "  getNormalizationLayer(),\n",
    "  Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  MaxPooling2D(),\n",
    "  Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  MaxPooling2D(),\n",
    "  Conv2D(64, 3, padding='same', activation='relu'),\n",
    "  MaxPooling2D(),\n",
    "  Conv2D(64, 3, padding='same', activation='relu'),\n",
    "  MaxPooling2D(),\n",
    "  Dropout(0.2),\n",
    "  Flatten(),\n",
    "  Dense(512, activation='relu'),\n",
    "  Dense(512, activation='relu'),\n",
    "  Dense(256, activation='relu'),\n",
    "  Dense(128, activation='relu'),\n",
    "  Dense(len(CLASS_NAMES), name=\"outputs\")\n",
    "], name = \"Second_Model\")\n",
    "\n",
    "'''# > 0.75 Val accuracy\n",
    "second_model = Sequential([\n",
    "  getAugmentationLayers(),\n",
    "  getNormalizationLayer(),\n",
    "  Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  MaxPooling2D(),\n",
    "  Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  MaxPooling2D(),\n",
    "  Conv2D(64, 3, padding='same', activation='relu'),\n",
    "  MaxPooling2D(),\n",
    "  Conv2D(64, 3, padding='same', activation='relu'),\n",
    "  MaxPooling2D(),\n",
    "  Conv2D(128, 3, padding='same', activation='relu'),\n",
    "  MaxPooling2D(),\n",
    "  Dropout(0.2),\n",
    "  Flatten(),\n",
    "  Dense(512, activation='relu'),\n",
    "  Dense(512, activation='relu'),\n",
    "  Dense(256, activation='relu'),\n",
    "  Dense(128, activation='relu'),\n",
    "  Dense(len(CLASS_NAMES), name=\"outputs\")\n",
    "], name = \"Second_Model\")\n",
    "\n",
    "second_model = Sequential([\n",
    "  getAugmentationLayers(),\n",
    "  getNormalizationLayer(),\n",
    "  Conv2D(32, (3, 3), activation='relu'),\n",
    "  MaxPooling2D((2, 2)),\n",
    "  Conv2D(64, (3, 3), activation='relu'),\n",
    "  MaxPooling2D((2, 2)),\n",
    "  Conv2D(128, (3, 3), activation='relu'),\n",
    "  MaxPooling2D((2, 2)),\n",
    "  Flatten(),\n",
    "  Dense(256, activation='relu'),\n",
    "  Dropout(0.5),\n",
    "  Dense(len(CLASS_NAMES), activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "second_model = Sequential([\n",
    "  getAugmentationLayers(),\n",
    "  getNormalizationLayer(),\n",
    "  Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  MaxPooling2D(),\n",
    "  Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  MaxPooling2D(),\n",
    "  Conv2D(64, 3, padding='same', activation='relu'),\n",
    "  MaxPooling2D(),\n",
    "  Conv2D(64, 3, padding='same', activation='relu'),\n",
    "  MaxPooling2D(),\n",
    "  Conv2D(128, 3, padding='same', activation='relu'),\n",
    "  MaxPooling2D(),\n",
    "  Dropout(0.2),\n",
    "  Flatten(),  \n",
    "  Dense(512, activation='relu'),\n",
    "  Dense(256, activation='relu'),\n",
    "  Dense(128, activation='relu'),\n",
    "  Dense(64, activation='relu'),\n",
    "  Dense(len(CLASS_NAMES), name=\"outputs\")\n",
    "], name = \"Second_Model\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "second_model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "second_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS_SECOND = 20\n",
    "second_history = second_model.fit(\n",
    "  train_dataset,\n",
    "  validation_data= validation_dataset,\n",
    "  epochs=EPOCHS_SECOND\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = second_history.history['accuracy']\n",
    "val_acc = second_history.history['val_accuracy']\n",
    "\n",
    "loss = second_history.history['loss']\n",
    "val_loss = second_history.history['val_loss']\n",
    "\n",
    "epochs_range = range(EPOCHS_SECOND)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Extract true labels from the validation dataset\n",
    "y_true = []\n",
    "for images, labels in validation_dataset:\n",
    "    y_true.extend(labels.numpy())  # Convert the labels from tensor to numpy\n",
    "\n",
    "# Step 2: Get predictions from the model (do this once, not twice)\n",
    "predictions = second_model.predict(validation_dataset)\n",
    "\n",
    "# Convert predictions to class labels (assuming probabilities)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Step 3: Generate the classification report\n",
    "infos_2 = classification_report(y_true, predicted_labels)\n",
    "print(infos_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "y_pred = first_model.predict(validation_dataset)\n",
    "y_pred_class = np.argmax(y_pred, axis=1)\n",
    "\n",
    "y_true = []\n",
    "for images, labels in validation_dataset:\n",
    "    y_true.extend(labels.numpy())\n",
    "\n",
    "#LABELS = {'Bicycle': 0, 'Bridge': 1, 'Bus': 2, 'Car': 3, 'Chimney': 4, 'Crosswalk': 5, 'Hydrant':6, 'Motorcycle':7, 'Other':8, 'Palm':9, 'Stair':10, 'Traffic Light':11}\n",
    "#classes = ['Bicycle', 'Bridge', 'Bus', 'Car', 'Chimney', 'Crosswalk', 'Hydrant', 'Motorcycle', 'Other', 'Palm', 'Stair', 'Traffic Light']\n",
    "\n",
    "LABELS = {'Bicycle': 0, 'Bridge': 1, 'Bus': 2, 'Car': 3, 'Crosswalk': 4, 'Hydrant':5, 'Palm':7, 'Traffic Light':8}\n",
    "classes = ['Bicycle', 'Bridge', 'Bus', 'Car', 'Crosswalk', 'Hydrant', 'Palm', 'Traffic Light']\n",
    "\n",
    "report = classification_report(y_true, y_pred_class, target_names=classes)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Générer une matrice de confusion\n",
    "cm = confusion_matrix(y_true, y_pred_class)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "cm_normalized = np.round(cm_normalized, decimals=2)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_normalized,\n",
    "                              display_labels=classes)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "good_pred = 0\n",
    "bad_pred = 0\n",
    "\n",
    "for i in range(1,1000):\n",
    "    \n",
    "    img_path = getImagesDataFolderPath() +\"/Bicycle/Bicycle (\"+ str(i) +\").png\"\n",
    "\n",
    "    #load image\n",
    "    try:\n",
    "\n",
    "        img = tf.keras.utils.load_img(\n",
    "            img_path, target_size=(IMG_DIMENSIONS[0], IMG_DIMENSIONS[1])\n",
    "        )\n",
    "\n",
    "        # image to array\n",
    "        img_array = tf.keras.utils.img_to_array(img)\n",
    "        img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "\n",
    "\n",
    "        #make prediction\n",
    "        predictions = second_model.predict(img_array)\n",
    "        #get score\n",
    "        score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "        if CLASS_NAMES[np.argmax(score)] == \"Bicycle\":\n",
    "            good_pred += 1\n",
    "        else:\n",
    "            bad_pred += 1\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    #print(\"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    #    .format(CLASS_NAMES[np.argmax(score)], 100 * np.max(score)))\n",
    "\n",
    "print(good_pred)\n",
    "print(bad_pred)\n",
    "print(good_pred/(good_pred+bad_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "img_path = getImagesDataFolderPath() +\"/Bicycle/Bicycle (778).png\"\n",
    "#img_path = getImagesDataFolderPath() +\"/Bridge/Bridge (112).png\"\n",
    "#img_path = getImagesDataFolderPath() +\"/Chimney/Chimney (28).png\"\n",
    "#img_path = getImagesDataFolderPath() +\"/Other/Other (903).png\"\n",
    "#img_path = getImagesDataFolderPath() +\"/Stair/Stair (1560).png\"\n",
    "#img_path = getImagesDataFolderPath() +\"/Car/Car (72).png\"\n",
    "\n",
    "#load image\n",
    "img = tf.keras.utils.load_img(\n",
    "    img_path, target_size=(IMG_DIMENSIONS[0], IMG_DIMENSIONS[1])\n",
    ")\n",
    "\n",
    "# image to array\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "\n",
    "\n",
    "#make prediction\n",
    "predictions = second_model.predict(img_array)\n",
    "#get score\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "      .format(CLASS_NAMES[np.argmax(score)], 100 * np.max(score)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bicycle:        709     70      0.91\n",
    "Bus:            37      1171    0.03\n",
    "Bridge:         4       528     0.007\n",
    "Car:            35      496     0.066\n",
    "Crosswalk:      37      962     0.037\n",
    "Hydrant:        5       947     0.005\n",
    "Palm:           6       905     0.007\n",
    "Traffic Light:  13      778     0.016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Testing with Resnet layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### resnet50 \n",
    "Premier resnet, nécessite une fonction de preprocessing pour adapter les images en entrées pour être adapté aux images sur lequel resnet a été entrainé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense,Dropout\n",
    "from tensorflow.keras.applications import ResNet101V2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "first_model = Sequential([\n",
    "  getAugmentationLayers(),\n",
    "  getNormalizationLayer(),\n",
    "  ResNet101V2(include_top=False, input_shape=(120,120,3)),\n",
    "  MaxPooling2D(),\n",
    "  Dropout(0.2),\n",
    "  Flatten(),\n",
    "  Dense(128, activation='relu'),\n",
    "  Dense(len(CLASS_NAMES), name=\"outputs\")\n",
    "], name = \"First_Model\")\n",
    "\n",
    "callback = EarlyStopping(patience=2)\n",
    "\n",
    "first_model.compile(optimizer='adam',\n",
    "              loss= SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "first_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### resnet101 \n",
    "Légèrement plus efficace mais plus long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense,Dropout\n",
    "from tensorflow.keras.applications import ResNet50V2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "first_model = Sequential([\n",
    "  getAugmentationLayers(),\n",
    "  getNormalizationLayer(),\n",
    "  ResNet50V2(include_top=False, input_shape=(120,120,3)),\n",
    "  MaxPooling2D(),\n",
    "  Dropout(0.2),\n",
    "  Flatten(),\n",
    "  Dense(128, activation='relu'),\n",
    "  Dense(len(CLASS_NAMES), name=\"outputs\")\n",
    "], name = \"First_Model\")\n",
    "\n",
    "callback = EarlyStopping(patience=2)\n",
    "\n",
    "first_model.compile(optimizer='adam',\n",
    "              loss= SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "first_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from tensorflow.image import decode_jpeg, resize\n",
    "\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "IMG_DIMENSIONS = (120, 120) # pixels per pixels\n",
    "SEED_RANDOM = 123\n",
    "\n",
    "VALIDATION_RATIO = 0.2\n",
    "\n",
    "def getDatasets(batch_size=BATCH_SIZE,\n",
    "                img_dims=IMG_DIMENSIONS,\n",
    "                validation_ratio=VALIDATION_RATIO,\n",
    "                seed=SEED_RANDOM) -> tuple:\n",
    "  train = image_dataset_from_directory(\n",
    "    getImagesDataFolderPath(),\n",
    "    validation_split= validation_ratio,\n",
    "    subset= \"training\",\n",
    "\n",
    "    seed=       seed,\n",
    "    image_size= img_dims,\n",
    "    batch_size= batch_size)\n",
    "\n",
    "  validation = image_dataset_from_directory(\n",
    "    getImagesDataFolderPath(),\n",
    "    validation_split= validation_ratio,\n",
    "    subset= \"validation\",\n",
    "\n",
    "    seed=       seed,\n",
    "    image_size= img_dims,\n",
    "    batch_size= batch_size)\n",
    "\n",
    "  return train, validation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_dataset, validation_dataset = getDatasets()\n",
    "\n",
    "train_dataset= train_dataset.map(preprocess_data_for_resnet).cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "validation_dataset= validation_dataset.map(preprocess_data_for_resnet).cache().prefetch(buffer_size=AUTOTUNE)\n",
    "EPOCHS_FIRST= 15\n",
    "\n",
    "first_history = first_model.fit(\n",
    "  train_dataset,\n",
    "  validation_data=validation_dataset,\n",
    "  epochs=EPOCHS_FIRST\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Essai Early stopping\n",
    "L'entrainement étant assez instable il est difficile d'utiliser la fonctionalité early stopping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "train_dataset, validation_dataset = getDatasets()\n",
    "\n",
    "train_dataset= train_dataset.map(preprocess_data_for_resnet).cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "validation_dataset= validation_dataset.map(preprocess_data_for_resnet).cache().prefetch(buffer_size=AUTOTUNE)\n",
    "EPOCHS_FIRST= 15\n",
    "\n",
    "first_history = first_model.fit(\n",
    "  train_dataset,\n",
    "  callbacks=[callback],\n",
    "  validation_data=validation_dataset,\n",
    "  epochs=EPOCHS_FIRST\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "TP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
