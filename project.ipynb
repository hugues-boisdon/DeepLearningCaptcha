{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lx4jEs_D3ATI"
   },
   "source": [
    "# ***Google Captcha image recognition***\n",
    "**A Deep Learning Project using TensorFlow**\n",
    "\n",
    "*by Emma Begard, Augustin Bouveau, Gabin Jobert--Rollin, Hugues Boisdon*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I Data **Fetching**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our source Dataset can be found at : *https://www.kaggle.com/datasets/mikhailma/test-dataset*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***credits : Mike Mazurov***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.1 **Dowloading** Dataset Files from ***KaggleHub*** Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VFMAVdi34rRi",
    "outputId": "04b8fd29-b158-4cd8-a1f6-f691de58d0d3"
   },
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "DATA_FOLDER_PATH_IF_CACHED = kagglehub.dataset_download(\"mikhailma/test-dataset\")\n",
    "print(\"Path to dataset files in cache:\", DATA_FOLDER_PATH_IF_CACHED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data files are downloaded in the Users **cache** by default.\n",
    "If the data source folder is moved, ***please update*** the following *path variable*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER_PATH_IF_MOVED = \"\" # Data folder path if data was moved since download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataSourceFolderPath() -> str:\n",
    "    return DATA_FOLDER_PATH_IF_CACHED if DATA_FOLDER_PATH_IF_MOVED == \"\" else DATA_FOLDER_PATH_IF_MOVED\n",
    "\n",
    "def getImagesDataFolderPath() -> str:\n",
    "    return getDataSourceFolderPath() +\"/Google_Recaptcha_V2_Images_Dataset/images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "path = DATA_FOLDER_PATH_IF_CACHED if DATA_FOLDER_PATH_IF_MOVED == \"\" else DATA_FOLDER_PATH_IF_MOVED\n",
    "Image.open(getImagesDataFolderPath() +\"/Bicycle/Bicycle (1).png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.2 First Try at **Loading** the Training and Validation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "IMG_DIMENSIONS = (120, 120) # pixels per pixels\n",
    "SEED_RANDOM = 123\n",
    "\n",
    "VALIDATION_RATIO = 0.2\n",
    "\n",
    "def getDatasets(batch_size=BATCH_SIZE, \n",
    "                img_dims=IMG_DIMENSIONS, \n",
    "                validation_ratio=VALIDATION_RATIO, \n",
    "                seed=SEED_RANDOM) -> tuple:\n",
    "  train = image_dataset_from_directory(\n",
    "    getImagesDataFolderPath(),\n",
    "    validation_split= validation_ratio,\n",
    "    subset= \"training\",\n",
    "    \n",
    "    seed=       seed,\n",
    "    image_size= img_dims,\n",
    "    batch_size= batch_size)\n",
    "\n",
    "  validation = image_dataset_from_directory(\n",
    "    getImagesDataFolderPath(),\n",
    "    validation_split= validation_ratio,\n",
    "    subset= \"validation\",\n",
    "    \n",
    "    seed=       seed,\n",
    "    image_size= img_dims,\n",
    "    batch_size= batch_size)\n",
    "  \n",
    "  return train, validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **keras** sublibrary of ***TensorFlow*** allow us to directly load our datasets (and ensure the size normalization of 120px per 120px for all images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, validation_dataset = getDatasets()\n",
    "\n",
    "CLASS_NAMES = train_dataset.class_names\n",
    "print(CLASS_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is going to be guessing between these classes for each image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Sample for Visualization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_dataset.take(1):\n",
    "  for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "    plt.title(CLASS_NAMES[labels[i]])\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II Data **Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Preprocessing layers & Optimizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 **Normalization Layer** for pixel values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Rescaling\n",
    "\n",
    "def getNormalizationLayer():\n",
    "    return Rescaling(1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pixel values will now be bound from 0 to 1 instead of 0 to 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Data Augmentation Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomZoom\n",
    "\n",
    "def getAugmentationLayers(maxRotation:float= 0.1, maxZoom:float= 0.1) -> Sequential:\n",
    "    return Sequential(\n",
    "    [\n",
    "        RandomFlip(\"horizontal\",\n",
    "                        input_shape=(IMG_DIMENSIONS[0],\n",
    "                                    IMG_DIMENSIONS[1],\n",
    "                                    3)),\n",
    "        RandomRotation(maxRotation),\n",
    "        RandomZoom(maxZoom),\n",
    "    ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for images, _ in train_dataset.take(1):\n",
    "  for i in range(9):\n",
    "    augmented_images = getAugmentationLayers()(images)\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 **Optimizations** of Data memory caching, availibity and randomization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.data import AUTOTUNE\n",
    "\n",
    "train_dataset_opti        = train_dataset.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "validation_dataset_opti   = validation_dataset.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Organizing & Rebalancing the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Creating sub folders for test, train and validation for the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "def split_dataset_by_class(source_dir, train_dir, val_dir, test_dir, train_size=0.7, val_size=0.2, test_size=0.1, min_images=10):\n",
    "    # Ensure the directories for train, validation, and test exist\n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "    os.makedirs(val_dir, exist_ok=True)\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "    # Traverse each subdirectory (class folder) in the source directory\n",
    "    for class_name in os.listdir(source_dir):\n",
    "        class_folder = os.path.join(source_dir, class_name)\n",
    "        \n",
    "        # Skip non-directories (just in case there are files in the source dir)\n",
    "        if not os.path.isdir(class_folder):\n",
    "            continue\n",
    "\n",
    "        # Get all image files in the class folder\n",
    "        image_files = [os.path.join(class_folder, file) for file in os.listdir(class_folder)\n",
    "                       if file.lower().endswith(('png', 'jpg', 'jpeg'))]  # Adjust for your image file extensions\n",
    "\n",
    "        # Skip the class if there are fewer than min_images\n",
    "        if len(image_files) < min_images:\n",
    "            print(f\"Skipping class {class_name} because it has fewer than {min_images} images.\")\n",
    "            continue\n",
    "\n",
    "        # Create the same class subdirectories in train, val, and test directories\n",
    "        os.makedirs(os.path.join(train_dir, class_name), exist_ok=True)\n",
    "        os.makedirs(os.path.join(val_dir, class_name), exist_ok=True)\n",
    "        os.makedirs(os.path.join(test_dir, class_name), exist_ok=True)\n",
    "\n",
    "        # Shuffle the image files for randomness\n",
    "        random.shuffle(image_files)\n",
    "\n",
    "        # Calculate the number of images for each set\n",
    "        total_images = len(image_files)\n",
    "        train_count = int(total_images * train_size)\n",
    "        val_count = int(total_images * val_size)\n",
    "        test_count = total_images - train_count - val_count  # Remaining for test\n",
    "\n",
    "        # Split the images\n",
    "        train_images = image_files[:train_count]\n",
    "        val_images = image_files[train_count:train_count + val_count]\n",
    "        test_images = image_files[train_count + val_count:]\n",
    "\n",
    "        # Copy the images into the corresponding directories\n",
    "        def copy_images(image_list, target_dir):\n",
    "            for img_path in image_list:\n",
    "                shutil.copy(img_path, target_dir)\n",
    "\n",
    "        copy_images(train_images, os.path.join(train_dir, class_name))\n",
    "        copy_images(val_images, os.path.join(val_dir, class_name))\n",
    "        copy_images(test_images, os.path.join(test_dir, class_name))\n",
    "\n",
    "        print(f\"Class {class_name}: {train_count} for training, {val_count} for validation, {test_count} for testing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Balancing classes in the training folder and validation folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "def duplicate_images_randomly_per_folder(root_dir, target_count_per_folder, min_img=5, max_img=100):\n",
    "    # Traverse all subdirectories in the root directory\n",
    "    for root, subdirs, files in os.walk(root_dir):\n",
    "        # Skip the root directory itself\n",
    "        if root == root_dir:  \n",
    "            continue\n",
    "\n",
    "        # Print the current directory being processed (for debugging)\n",
    "        print(f\"Processing folder: {root}\")\n",
    "        \n",
    "        # Filter image files (png, jpg, jpeg)\n",
    "        image_files = [os.path.join(root, file) for file in files if file.lower().endswith(('png', 'jpg', 'jpeg'))]\n",
    "        current_count = len(image_files)\n",
    "        \n",
    "        # If the number of images is less than min_img, delete the folder\n",
    "        if current_count < min_img:\n",
    "            print(f\"Directory {root} has fewer than {min_img} images, deleting folder.\")\n",
    "            # Delete all the files in the folder before removing it\n",
    "            for file in image_files:\n",
    "                os.remove(file)\n",
    "            os.rmdir(root)  # Remove the folder itself\n",
    "            continue  # Skip to the next folder\n",
    "        \n",
    "        # If the number of images exceeds max_img, randomly delete images to meet max_img limit\n",
    "        if current_count > max_img:\n",
    "            print(f\"Directory {root} has more than {max_img} images, deleting excess images.\")\n",
    "            images_to_delete = current_count - max_img\n",
    "            random.shuffle(image_files)  # Shuffle to delete images randomly\n",
    "            for img_path in image_files[:images_to_delete]:\n",
    "                os.remove(img_path)  # Remove the selected images\n",
    "            current_count = max_img  # Update current count after deletion\n",
    "        \n",
    "        # If the current count is already greater than or equal to the target, skip duplication\n",
    "        if current_count >= target_count_per_folder:\n",
    "            print(f\"Directory {root} already has {current_count} images, no duplication needed.\")\n",
    "            continue\n",
    "        \n",
    "        # Calculate how many more images are needed\n",
    "        images_needed = target_count_per_folder - current_count\n",
    "        print(f\"Duplicating {images_needed} images randomly in {root}.\")\n",
    "        \n",
    "        # Duplicate images randomly until the target count is reached\n",
    "        while images_needed > 0:\n",
    "            random.shuffle(image_files)  # Shuffle the list of image files\n",
    "            for img_path in image_files:\n",
    "                if images_needed <= 0:\n",
    "                    break\n",
    "                \n",
    "                # Load the image\n",
    "                img = Image.open(img_path)\n",
    "                \n",
    "                # Create a unique name for the duplicated image\n",
    "                folder_name, img_name = os.path.split(img_path)\n",
    "                duplicated_img_name = f\"{os.path.splitext(img_name)[0]}_dup{images_needed}{os.path.splitext(img_name)[1]}\"\n",
    "                duplicated_img_path = os.path.join(folder_name, duplicated_img_name)\n",
    "                \n",
    "                # Save the duplicated image with a new name\n",
    "                img.save(duplicated_img_path)\n",
    "\n",
    "                # Decrement the remaining number of images needed\n",
    "                images_needed -= 1\n",
    "\n",
    "        print(f\"Duplicated images in {root} to reach {target_count_per_folder} images.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Image preprocesssing for Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet import preprocess_input\n",
    "\n",
    "def preprocess_data_for_resnet(image, label):\n",
    "    image = preprocess_input(image)  # Apply preprocessing\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III Our differents approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Naive CNN models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III.1.A Defining and compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "first_model = Sequential([\n",
    "  Conv2D(16, 3, padding='same', activation='relu'),\n",
    "  MaxPooling2D(),\n",
    "  Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  MaxPooling2D(),\n",
    "  Conv2D(64, 3, padding='same', activation='relu'),\n",
    "  MaxPooling2D(),\n",
    "  Flatten(),\n",
    "  Dense(128, activation='relu'),\n",
    "  Dense(len(CLASS_NAMES), name=\"outputs\")\n",
    "], name = \"First_Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "first_model.compile(optimizer='adam',\n",
    "              loss= SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "first_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III.1.B Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS_FIRST= 10\n",
    "\n",
    "first_history = first_model.fit(\n",
    "  train_dataset,\n",
    "  validation_data=validation_dataset,\n",
    "  epochs=EPOCHS_FIRST\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III.1.C Visualizing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = first_history.history['accuracy']\n",
    "val_acc = first_history.history['val_accuracy']\n",
    "\n",
    "loss = first_history.history['loss']\n",
    "val_loss = first_history.history['val_loss']\n",
    "\n",
    "epochs_range = range(EPOCHS_FIRST)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import argmax\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "tf.math.confusion_matrix(\n",
    "    labels,\n",
    "    predictions,\n",
    "    num_classes=None,\n",
    "    weights=None,\n",
    "    dtype=tf.dtypes.int32,\n",
    "    name=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Extract true labels from the validation dataset\n",
    "y_true = []\n",
    "for images, labels in validation_dataset:\n",
    "    y_true.extend(labels.numpy())  # Convert the labels from tensor to numpy\n",
    "\n",
    "# Step 2: Get predictions from the model (do this once, not twice)\n",
    "predictions = first_model.predict(validation_dataset)\n",
    "\n",
    "# Convert predictions to class labels (assuming probabilities)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Step 3: Generate the classification report\n",
    "infos_2 = classification_report(y_true, predicted_labels)\n",
    "print(infos_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III.2.A Defining and compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "second_model = Sequential([\n",
    "  getAugmentationLayers(),\n",
    "  getNormalizationLayer(),\n",
    "  Conv2D(16, 3, padding='same', activation='relu'),\n",
    "  MaxPooling2D(),\n",
    "  Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  MaxPooling2D(),\n",
    "  Conv2D(64, 3, padding='same', activation='relu'),\n",
    "  MaxPooling2D(),\n",
    "  Dropout(0.2),\n",
    "  Flatten(),\n",
    "  Dense(128, activation='relu'),\n",
    "  Dense(len(CLASS_NAMES), name=\"outputs\")\n",
    "], name = \"Second_Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "second_model.compile(optimizer='adam',\n",
    "              loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "second_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III.2.B Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS_SECOND = 15\n",
    "second_history = second_model.fit(\n",
    "  train_dataset,\n",
    "  validation_data= validation_dataset,\n",
    "  epochs=EPOCHS_SECOND\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Extract true labels from the validation dataset\n",
    "y_true = []\n",
    "for images, labels in validation_dataset:\n",
    "    y_true.extend(labels.numpy())  # Convert the labels from tensor to numpy\n",
    "\n",
    "# Step 2: Get predictions from the model (do this once, not twice)\n",
    "predictions = second_model.predict(validation_dataset)\n",
    "\n",
    "# Convert predictions to class labels (assuming probabilities)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Step 3: Generate the classification report\n",
    "infos_2 = classification_report(y_true, predicted_labels)\n",
    "print(infos_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III.2.C Visualizing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = second_history.history['accuracy']\n",
    "val_acc = second_history.history['val_accuracy']\n",
    "\n",
    "loss = second_history.history['loss']\n",
    "val_loss = second_history.history['val_loss']\n",
    "\n",
    "epochs_range = range(EPOCHS_SECOND)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.image import decode_jpeg, resize\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "def preprocess_data_for_resnet(image, label):\n",
    "    image = preprocess_input(image)  # Apply preprocessing\n",
    "    return image, label\n",
    "train_dataset, validation_dataset = getDatasets()\n",
    "\n",
    "train_dataset= train_dataset.map(preprocess_data_for_resnet).cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "validation_dataset= validation_dataset.map(preprocess_data_for_resnet).cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Testing CNN models with new preprocessings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Loading the new datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_directory = getImagesDataFolderPath()\n",
    "train_directory = './datas/train'  \n",
    "val_directory = './datas/validation' \n",
    "test_directory = './datas/test' \n",
    "\n",
    "# Split the dataset, skipping classes with fewer than 500 images\n",
    "split_dataset_by_class(source_directory, train_directory, val_directory, test_directory, min_images=500)\n",
    "\n",
    "# Define your target number of images for each subfolder\n",
    "target_number_per_folder = 1500  # Set your target number of images per folder\n",
    "\n",
    "# Set your root image directory (which contains subfolders like 'cars', 'bicycles', etc.)\n",
    "root_image_directory = getImagesDataFolderPath() \n",
    "\n",
    "# for training \n",
    "duplicate_images_randomly_per_folder(\"./datas/training\", 1500, min_img=500, max_img=1500)\n",
    "# for validation \n",
    "duplicate_images_randomly_per_folder(\"./datas/validation\", 1500*0.2, min_img=500*0.2, max_img=1500)\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = image_dataset_from_directory(\n",
    " \"./datas/train\" ,\n",
    "  label_mode='int',\n",
    "  image_size= IMG_DIMENSIONS,\n",
    "  batch_size= BATCH_SIZE)\n",
    "\n",
    "validation_dataset = image_dataset_from_directory(\n",
    "  \"./datas/validation\" ,\n",
    "  label_mode='int',\n",
    "  image_size= IMG_DIMENSIONS,\n",
    "  batch_size= BATCH_SIZE)\n",
    "\n",
    "test_dataset = image_dataset_from_directory(\n",
    "  \"./datas/test\" ,\n",
    "  label_mode='int',\n",
    "  seed=       SEED_RANDOM,\n",
    "  image_size= IMG_DIMENSIONS,\n",
    "  batch_size= BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, GlobalAveragePooling2D\n",
    "\n",
    "model_2_1 = Sequential([\n",
    "    getAugmentationLayers(),\n",
    "    getNormalizationLayer(),\n",
    "    Conv2D(16, (3, 3), padding='same', activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
    "    GlobalAveragePooling2D(),  # Summarizes feature maps\n",
    "    Dropout(0.3),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(len(CLASS_NAMES), activation='softmax', name=\"outputs\")\n",
    "], name=\"model_1\")\n",
    "\n",
    "\n",
    "model_2_2 = Sequential([\n",
    "    getAugmentationLayers(),\n",
    "    getNormalizationLayer(),\n",
    "    Conv2D(16, (3, 3), padding='same', activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
    "    MaxPooling2D((2, 2)),  # Add additional MaxPooling layer\n",
    "    Conv2D(256, (3, 3), padding='same', activation='relu'),  # Additional convolutional layer\n",
    "    MaxPooling2D((2, 2)),\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dropout(0.3),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(len(CLASS_NAMES), activation='softmax', name=\"outputs\")\n",
    "], name=\"model_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Testing with Resnet layers"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
