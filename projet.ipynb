{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4oOUc1qi91oq"
   },
   "source": [
    "Ceci n'est pas un commentaire\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lx4jEs_D3ATI"
   },
   "source": [
    "# ***Google Captcha image recognition***\n",
    "**A Deep Learning Project using TensorFlow**\n",
    "\n",
    "*by Emma Begard, Augustin Bouveau, Bagin Jobert-Rollin, Hugues Boisdon*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I Data **Fetching**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our source Dataset can be found at : *https://www.kaggle.com/datasets/mikhailma/test-dataset*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*credits : **Mike Mazurov***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.1 **Dowloading** Data Files from ***KaggleHub*** Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VFMAVdi34rRi",
    "outputId": "04b8fd29-b158-4cd8-a1f6-f691de58d0d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files in cache: C:\\Users\\Development\\.cache\\kagglehub\\datasets\\mikhailma\\test-dataset\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "DATA_FOLDER_PATH_IF_CACHED = kagglehub.dataset_download(\"mikhailma/test-dataset\")\n",
    "print(\"Path to dataset files in cache:\", DATA_FOLDER_PATH_IF_CACHED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data files are downloaded in the Users **cache** by default.\n",
    "If the data source folder is moved, ***please update*** the following *path variable*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER_PATH_IF_MOVED = \"\" # Data folder path if data was moved since download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.2 **Loading** Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.2.A *Functions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def loadImagesFromDirectory(directoryPath:str) -> pd.DataFrame:\n",
    "    data: dict[str:list] = {\"Image\": [], \"Label\": []}\n",
    "    \n",
    "    for root, directories, files in os.walk(directoryPath):\n",
    "        for f in files:\n",
    "            subFolderName = f.split(\" \")[0]\n",
    "            if subFolderName == \"Cross\"  : subFolderName = \"Crosswalk\"\n",
    "            if subFolderName == \"Tlight\" : subFolderName = \"Traffic Light\"\n",
    "\n",
    "            data[\"Image\"].append(Image.open(directoryPath+ \"/\" + subFolderName + \"/\"+ f).convert(\"RGB\"))\n",
    "            data[\"Label\"].append(subFolderName)\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.2.B Loading **raw** image data in a *Dataframe*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;PIL.Image.Image image mode=RGB size=120x120 a...</td>\n",
       "      <td>Bicycle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;PIL.Image.Image image mode=RGB size=120x120 a...</td>\n",
       "      <td>Bicycle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;PIL.Image.Image image mode=RGB size=120x120 a...</td>\n",
       "      <td>Bicycle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;PIL.Image.Image image mode=RGB size=120x120 a...</td>\n",
       "      <td>Bicycle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;PIL.Image.Image image mode=RGB size=120x120 a...</td>\n",
       "      <td>Bicycle</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Image    Label\n",
       "0  <PIL.Image.Image image mode=RGB size=120x120 a...  Bicycle\n",
       "1  <PIL.Image.Image image mode=RGB size=120x120 a...  Bicycle\n",
       "2  <PIL.Image.Image image mode=RGB size=120x120 a...  Bicycle\n",
       "3  <PIL.Image.Image image mode=RGB size=120x120 a...  Bicycle\n",
       "4  <PIL.Image.Image image mode=RGB size=120x120 a...  Bicycle"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgFolderPath = DATA_FOLDER_PATH_IF_CACHED+\"/Google_Recaptcha_V2_Images_Dataset/images\" if DATA_FOLDER_PATH_IF_MOVED == \"\" else DATA_FOLDER_PATH_IF_MOVED+\"/Google_Recaptcha_V2_Images_Dataset/images\"\n",
    "\n",
    "rawImagesDf = loadImagesFromDirectory(imgFolderPath)\n",
    "rawImagesDf.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II Data **Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.1 **Cleaning** Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.1.A *Images Size* Normalization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### II.1.A.a funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkImagesSizeInDataframe(df:pd.DataFrame) -> None:\n",
    "    sizeCounts = {}\n",
    "    for image in df[\"Image\"]:\n",
    "        if not(image.size in sizeCounts):\n",
    "            sizeCounts[image.size] = 1\n",
    "        else :\n",
    "            sizeCounts[image.size] += 1\n",
    "    print(f\"Sizes found in Dataframe : {sizeCounts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageOps\n",
    "from copy import deepcopy\n",
    "\n",
    "def normalizeImageSizes(df:pd.DataFrame, imgTargetSize:tuple[int,int] = (120, 120)) -> pd.DataFrame:\n",
    "    dataframe = deepcopy(df)\n",
    "    numOfCropped = 0; numOfExpandeds = 0\n",
    "    for index, row in dataframe.iterrows():\n",
    "        width  = row[\"Image\"].width\n",
    "        height = row[\"Image\"].height\n",
    "        \n",
    "        noPaddings  = [0   , 0  ,     0,      0]\n",
    "        noCroppings = [0   , 0  , width, height]\n",
    "                    #  Left, Top, Right, Bottom\n",
    "        \n",
    "        paddings  = deepcopy(noPaddings)\n",
    "        croppings = deepcopy(noCroppings)\n",
    "        \n",
    "        if width < imgTargetSize[0]:\n",
    "            pixelsToAdd = imgTargetSize[0] - width\n",
    "            paddings[0] = pixelsToAdd  // 2\n",
    "            paddings[2] = (pixelsToAdd - pixelsToAdd  // 2)\n",
    "        elif width > imgTargetSize[0]:\n",
    "            pixelsToCrop = width - imgTargetSize[0]\n",
    "            croppings[0] = pixelsToCrop // 2\n",
    "            croppings[2] = width - (pixelsToCrop-pixelsToCrop // 2)\n",
    "        \n",
    "        if height < imgTargetSize[1]:\n",
    "            pixelsToAdd = imgTargetSize[1] - height\n",
    "            paddings[1] = pixelsToAdd  // 2\n",
    "            paddings[3] = (pixelsToAdd - pixelsToAdd  // 2)\n",
    "        elif height > imgTargetSize[1]:\n",
    "            pixelsToCrop = height - imgTargetSize[1]\n",
    "            croppings[1] = pixelsToCrop // 2\n",
    "            croppings[3] = height - (pixelsToCrop-pixelsToCrop // 2)\n",
    "        \n",
    "        if paddings != noPaddings:\n",
    "            row[\"Image\"] = ImageOps.expand(row[\"Image\"], border=tuple(paddings), fill='black')\n",
    "            numOfExpandeds += 1\n",
    "        if croppings != noCroppings:\n",
    "            row[\"Image\"] = ImageOps.crop(row[\"Image\"], border=tuple(paddings))\n",
    "            numOfCropped += 1\n",
    "    print(f\"{numOfCropped} images were cropped to {imgTargetSize}!\")\n",
    "    print(f\"{numOfExpandeds} images were expanded to {imgTargetSize}!\")\n",
    "    return dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### II.1.A.b Normalizing process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sizes found in Dataframe : {(120, 120): 10705, (100, 100): 1025}\n",
      "0 images were cropped to (120, 120)!\n",
      "1025 images were expanded to (120, 120)!\n",
      "Sizes found in Dataframe : {(120, 120): 11730}\n"
     ]
    }
   ],
   "source": [
    "# Initial Check\n",
    "checkImagesSizeInDataframe(rawImagesDf)\n",
    "\n",
    "normalizedSizeImagesDf = normalizeImageSizes(rawImagesDf)\n",
    "\n",
    "# Final Check\n",
    "checkImagesSizeInDataframe(normalizedSizeImagesDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.1.B Transforming Images to *pixel value Arrays*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "\n",
    "def imagesToPixelArrays(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    dataframe = deepcopy(df)\n",
    "    for id, row in dataframe.iterrows():\n",
    "        row[\"Image\"] = np.array(row[\"Image\"]).astype(np.float32)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[[117.0, 114.0, 104.0], [114.0, 110.0, 101.0]...</td>\n",
       "      <td>Bicycle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[[54.0, 75.0, 67.0], [58.0, 76.0, 69.0], [66....</td>\n",
       "      <td>Bicycle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[[38.0, 33.0, 27.0], [37.0, 34.0, 28.0], [37....</td>\n",
       "      <td>Bicycle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[[49.0, 49.0, 51.0], [55.0, 56.0, 58.0], [72....</td>\n",
       "      <td>Bicycle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[[135.0, 128.0, 125.0], [143.0, 130.0, 129.0]...</td>\n",
       "      <td>Bicycle</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Image    Label\n",
       "0  [[[117.0, 114.0, 104.0], [114.0, 110.0, 101.0]...  Bicycle\n",
       "1  [[[54.0, 75.0, 67.0], [58.0, 76.0, 69.0], [66....  Bicycle\n",
       "2  [[[38.0, 33.0, 27.0], [37.0, 34.0, 28.0], [37....  Bicycle\n",
       "3  [[[49.0, 49.0, 51.0], [55.0, 56.0, 58.0], [72....  Bicycle\n",
       "4  [[[135.0, 128.0, 125.0], [143.0, 130.0, 129.0]...  Bicycle"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pixelArraysDf = imagesToPixelArrays(normalizedSizeImagesDf)\n",
    "\n",
    "pixelArraysDf.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.1.C *Pixel values* Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizePixelValues(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    dataframe = deepcopy(df)\n",
    "    for id, row in dataframe.iterrows():\n",
    "        row[\"Image\"] /= 255\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[[0.45882353, 0.44705883, 0.40784314], [0.447...</td>\n",
       "      <td>Bicycle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[[0.21176471, 0.29411766, 0.2627451], [0.2274...</td>\n",
       "      <td>Bicycle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[[0.14901961, 0.12941177, 0.105882354], [0.14...</td>\n",
       "      <td>Bicycle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[[0.19215687, 0.19215687, 0.2], [0.21568628, ...</td>\n",
       "      <td>Bicycle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[[0.5294118, 0.5019608, 0.49019608], [0.56078...</td>\n",
       "      <td>Bicycle</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Image    Label\n",
       "0  [[[0.45882353, 0.44705883, 0.40784314], [0.447...  Bicycle\n",
       "1  [[[0.21176471, 0.29411766, 0.2627451], [0.2274...  Bicycle\n",
       "2  [[[0.14901961, 0.12941177, 0.105882354], [0.14...  Bicycle\n",
       "3  [[[0.19215687, 0.19215687, 0.2], [0.21568628, ...  Bicycle\n",
       "4  [[[0.5294118, 0.5019608, 0.49019608], [0.56078...  Bicycle"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizedPixelArraysDf = normalizePixelValues(pixelArraysDf)\n",
    "\n",
    "normalizedPixelArraysDf.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.2 **Splitting** Data in *Training* and *Testing* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[101], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      3\u001b[0m train_images, test_images, train_labels, test_labels \u001b[38;5;241m=\u001b[39m train_test_split(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto_numpy(), df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto_numpy(), test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m      5\u001b[0m train_images, test_images \u001b[38;5;241m=\u001b[39m train_images \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m, test_images \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(df[\"Image\"].to_numpy(), df[\"Label\"].to_numpy(), test_size=0.2, random_state=42)\n",
    "\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "\n",
    "print(train_images[:5])\n",
    "print(train_labels[:5])\n",
    "print(test_images[:5])\n",
    "print(test_labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#for index, row in train_images.iterrow():\n",
    "#    print(row)\n",
    "\n",
    "#train_images = train_images.values.reshape(-1,1)\n",
    "#print(train_images[:5])\n",
    "train_images = tf.convert_to_tensor(train_images, dtype=np.float32)\n",
    "print(train_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III **Designing** the *Model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pntlgWRL4IEU"
   },
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "\n",
    "\n",
    "def build_model(dense_neurons):\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.InputLayer((120,120,3)),\n",
    "        layers.Conv2D(32, (3,3), activation='relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(dense_neurons, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jyuDKx076oL9"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m BAT \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBAT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[0;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m     11\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(test_images, test_labels, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\augus\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\augus\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:108\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    106\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m    107\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray)."
     ]
    }
   ],
   "source": [
    "model = build_model(dense_neurons=64)\n",
    "BAT = 64\n",
    "    # Train the model\n",
    "history = model.fit(\n",
    "    train_images,train_labels,\n",
    "    batch_size=BAT,\n",
    "    epochs=5,\n",
    "    verbose=2\n",
    ")\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(test_images, test_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r6IYD-fY8c9m"
   },
   "source": [
    "How to load images with tensorflow  : https://www.tensorflow.org/api_docs/python/tf/keras/utils/load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_mwq5Lly8gos"
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.load_img(\n",
    "    folderPath,\n",
    "    color_mode='rgb',\n",
    "    target_size=None,\n",
    "    interpolation='nearest',\n",
    "    keep_aspect_ratio=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
